{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO//ruQQUxlR8xkDf5KB+Ex",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoonZeee/textminer-pro/blob/main/textminer_pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# textminer-pro: 텍스트 전처리 & 분석 패키지\n",
        "## 이지훈\n",
        "\n",
        "이 프로젝트는 텍스트 데이터를 보다 효과적으로 처리하고 분석하기 위해 만든 Python 기반 경량 패키지입니다.  \n",
        "NLTK, scikit-learn, Sumy, langdetect 등을 활용하여 다음 기능을 제공합니다:\n",
        "\n",
        "- 불용어 제거 (`remove_stopwords`)\n",
        "- 키워드 추출 (`extract_keywords`)\n",
        "- 텍스트 요약 (`summarize_text`)\n",
        "- 언어 감지 (`detect_language`)\n",
        "\n"
      ],
      "metadata": {
        "id": "gUAkRkI82e_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 디렉터리 생성"
      ],
      "metadata": {
        "id": "pYmycKESidk5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jvH1x3TOh7vX"
      },
      "outputs": [],
      "source": [
        "!mkdir -p textminer_pro/textminer\n",
        "!mkdir -p textminer_pro/tests\n",
        "!mkdir -p textminer_pro/.github/workflows\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "!pip install sumy\n",
        "!pip install twine"
      ],
      "metadata": {
        "id": "vPedRt3OqAVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cleaner.py"
      ],
      "metadata": {
        "id": "iM0xdmQaiynv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile textminer_pro/textminer/cleaner.py\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "NLTK_PATH = \"/content/nltk_data\"\n",
        "nltk.data.path.append(NLTK_PATH)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered)\n",
        "\n",
        "def extract_keywords(text: str, top_n=5) -> list:\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    scores = zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[0])\n",
        "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    keywords = [word for word, score in sorted_scores[:top_n]]\n",
        "    return keywords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFfwb57piyKW",
        "outputId": "a788d6e2-70fa-459a-b099-176484bfca5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing textminer_pro/textminer/cleaner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cleaner.py 테스트코드"
      ],
      "metadata": {
        "id": "OMw-dm7JxdR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile textminer_pro/tests/test_cleaner.py\n",
        "import sys\n",
        "sys.path.append('/content/textminer_pro')\n",
        "import os\n",
        "import nltk\n",
        "from textminer import remove_stopwords, extract_keywords\n",
        "\n",
        "\n",
        "NLTK_PATH = \"/content/nltk_data\"\n",
        "os.environ[\"NLTK_DATA\"] = NLTK_PATH\n",
        "nltk.data.path.append(NLTK_PATH)\n",
        "\n",
        "\n",
        "nltk.download('punkt', download_dir=NLTK_PATH)\n",
        "nltk.download('punkt_tab', download_dir=NLTK_PATH)\n",
        "nltk.download('stopwords', download_dir=NLTK_PATH)\n",
        "\n",
        "text = \"This is a test sentence with simple words.\"\n",
        "print(\"Stopword 제거 결과:\")\n",
        "print(remove_stopwords(text))\n",
        "\n",
        "text2 = \"Machine learning is fun and powerful.\"\n",
        "print(\"키워드 추출 결과:\")\n",
        "print(extract_keywords(text2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FfjfaRHi8Zj",
        "outputId": "7d031b2f-6b4f-4ad8-b463-19e3e285b56b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing textminer_pro/tests/test_cleaner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## summarizer.py"
      ],
      "metadata": {
        "id": "-9l8xLM7qEAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile textminer_pro/textminer/summarizer.py\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "def summarize_text(text: str, num_sentences=2) -> str:\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    summary = summarizer(parser.document, num_sentences)\n",
        "    return \" \".join(str(sentence) for sentence in summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKfzaptMoKrz",
        "outputId": "9cf7e2d6-7ab5-4207-c941-5e4b3c2a7f0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing textminer_pro/textminer/summarizer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## detector.py"
      ],
      "metadata": {
        "id": "kZpie_0NxAGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile textminer_pro/textminer/detector.py\n",
        "from langdetect import detect\n",
        "\n",
        "def detect_language(text: str) -> str:\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"언어를 감지할 수 없습니다\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R992idzqKUv",
        "outputId": "974eddfd-6388-4032-d8e5-9228b8a7b302"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing textminer_pro/textminer/detector.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## init.py"
      ],
      "metadata": {
        "id": "dgnFsO0t0ygK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile textminer_pro/textminer/__init__.py\n",
        "from .cleaner import remove_stopwords, extract_keywords\n",
        "from .summarizer import summarize_text\n",
        "from .detector import detect_language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjvKUaz5qTYt",
        "outputId": "cf04256f-6d47-4751-8eef-0e2fb8953493"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing textminer_pro/textminer/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## detector.py 테스트코드"
      ],
      "metadata": {
        "id": "PjhYo04oxgx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile textminer_pro/tests/test_detector.py\n",
        "import sys\n",
        "sys.path.append('/content/textminer_pro')\n",
        "from textminer import detect_language\n",
        "\n",
        "text1 = \"This is an English sentence.\"\n",
        "text2 = \"이 문장은 한국어입니다.\"\n",
        "\n",
        "print(\"English 감지 결과:\", detect_language(text1))\n",
        "print(\"Korean 감지 결과:\", detect_language(text2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7u4NmLDoMkU",
        "outputId": "3bd675b0-592f-465b-f8b3-c258ae1e008a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing textminer_pro/tests/test_detector.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트코드 실행"
      ],
      "metadata": {
        "id": "jnYgQ68-xlUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 textminer_pro/tests/test_cleaner.py\n",
        "!python3 textminer_pro/tests/test_detector.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CVt9rVju7o5",
        "outputId": "548bc3c6-33d6-4c6f-bd66-7ffcd0a4d126"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Stopword 제거 결과:\n",
            "test sentence simple words .\n",
            "키워드 추출 결과:\n",
            "['fun', 'learning', 'machine', 'powerful']\n",
            "English 감지 결과: en\n",
            "Korean 감지 결과: ko\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile textminer_pro/setup.py\n",
        "from setuptools import setup, find_packages\n",
        "\n",
        "setup(\n",
        "    name='textminer-pro-jihoonLee',\n",
        "    version='0.1.0',\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        'nltk',\n",
        "        'scikit-learn',\n",
        "        'sumy',\n",
        "        'langdetect'\n",
        "    ],\n",
        "    author='Jihoon Lee',\n",
        "    author_email='dlwlgns7540@naver.com',\n",
        "    description='A simple text mining package with stopword removal, keyword extraction, summarization, and language detection.',\n",
        "    long_description=open('README.md').read(),\n",
        "    long_description_content_type='text/markdown',\n",
        "    url='https://github.com/hoonZeee/Oss_2025/tree/main/pypi',\n",
        "    classifiers=[\n",
        "        'Programming Language :: Python :: 3',\n",
        "        'License :: OSI Approved :: MIT License',\n",
        "    ],\n",
        "    python_requires='>=3.7',\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppWUVE_JqkJp",
        "outputId": "244f5852-c718-4454-e9d2-3308d245e54b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing textminer_pro/setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd textminer_pro\n",
        "!python setup.py sdist bdist_wheel"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Owh6uoHHrODC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /root/.pypirc\n",
        "[distutils]\n",
        "index-servers =\n",
        "    pypi\n",
        "\n",
        "[pypi]\n",
        "repository: https://upload.pypi.org/legacy/\n",
        "username: __token__\n",
        "password: 보안상 삭제합니다.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZKKTXr9sav3",
        "outputId": "c75b6e4e-e92c-4215-d2fb-c4b4b0111c3c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /root/.pypirc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!twine upload dist/*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzeUYxUvrlle",
        "outputId": "fcbe8fda-500a-410f-f3ad-e5639ab2b191"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading distributions to https://upload.pypi.org/legacy/\n",
            "Uploading textminer_pro_jihoonLee-0.1.0-py3-none-any.whl\n",
            "\u001b[2K\u001b[35m100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 kB\u001b[0m • \u001b[33m00:00\u001b[0m • \u001b[31m?\u001b[0m\n",
            "\u001b[?25hUploading textminer_pro_jihoonlee-0.1.0.tar.gz\n",
            "\u001b[2K\u001b[35m100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 kB\u001b[0m • \u001b[33m00:00\u001b[0m • \u001b[31m?\u001b[0m\n",
            "\u001b[?25h\n",
            "\u001b[32mView at:\u001b[0m\n",
            "https://pypi.org/project/textminer-pro-jihoonLee/0.1.0/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 최종 확인"
      ],
      "metadata": {
        "id": "VG9lY8ki2bHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -i https://test.pypi.org/simple/ textminer-pro"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0gohWv7DskXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textminer import remove_stopwords, extract_keywords, summarize_text, detect_language\n",
        "\n",
        "text = \"The OSS assignment is fun and educational, making it easier to extract key insights, summarize complex ideas, and even detect the language used!\"\n",
        "text2 = \"OSS 과제는 재미있고 유익해서, 텍스트 요약이나 키워드 추출, 언어 감지까지 쉽게 해볼 수 있다.\"\n",
        "\n",
        "\n",
        "print(remove_stopwords(text))\n",
        "print(extract_keywords(text))\n",
        "print(summarize_text(text))\n",
        "print(detect_language(text))\n",
        "\n",
        "print(remove_stopwords(text2))\n",
        "print(extract_keywords(text2))\n",
        "print(summarize_text(text2))\n",
        "print(detect_language(text2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfXoKicOsuY_",
        "outputId": "fb2ee270-981a-494f-8498-9b0c799f652e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OSS assignment fun educational , making easier extract key insights , summarize complex ideas , even detect language used !\n",
            "['assignment', 'complex', 'detect', 'easier', 'educational']\n",
            "The OSS assignment is fun and educational, making it easier to extract key insights, summarize complex ideas, and even detect the language used!\n",
            "en\n",
            "OSS 과제는 재미있고 유익해서 , 텍스트 요약이나 키워드 추출 , 언어 감지까지 쉽게 해볼 수 있다 .\n",
            "['oss', '감지까지', '과제는', '쉽게', '언어']\n",
            "\n",
            "ko\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Git 연동"
      ],
      "metadata": {
        "id": "wmL4xjok93wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "!git init\n",
        "!git rm -r --cached nltk_data sample_data .config"
      ],
      "metadata": {
        "id": "51EprEwN5Web"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .gitignore\n",
        "nltk_data/\n",
        "sample_data/\n",
        ".config/\n",
        "__pycache__/\n",
        "*.pyc\n",
        "*.ipynb_checkpoints\n",
        "*.egg-info/\n",
        "build/\n",
        "dist/\n"
      ],
      "metadata": {
        "id": "HkP4OX0S8kl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 커밋 푸쉬는 보안상 생략하겠습니다"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DZae5m0K8pCr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}